<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Constrained Policy Optimization (CPO)</h1>
    <p>An example project using Webpack, Babel, and Svelte. </p>
  </d-title>

  <d-article>

      <p>
        Welcome to our paper on constraint policy optimization (CPO). 
        In this article, we will discuss the CPO method for safe reinforcement learning, 
        which was presented in Achiam et al. <d-cite
        key="achiam2017constrained"></d-cite> 
        Inspired by the authors' idea, we use CPO to train two safe AI models for the OpenAI environments: CartPole and LunarLander
        To implement the CPO method, we apply the CG method to improve the algorithm. 
        Furthermore, we experimented different CG iterations to see if the algorithm could be improved.
        <br>
        Also, it was mentioned in the paper that the implementation of trust region could accelerate the model's training.
        Theoretically, a model with a large trust region would converge to the objective function faster but fit it poorly,
        and one with a small trust region would converge slower but fit the objective function better. Our hypothesis is: 
        For a trust region with a large size, the model will achieve higher rewards but might violate
        the constraint more often. On the other hand, a model with a small trust region would stay closer to the constraint,
        at the cost of achieving higher rewards.
        <br>
        For the implementation of the algorithm, we based our code on the work of Chaudhary, Sapana 
        (https://github.com/SapanaChaudhary/PyTorch-CPO) and on Achiam et al. <d-cite
        key="achiam2017constrained"></d-cite> 
      </p>
  
      <br>
      
  
      <h3>Table of contents</h3>
      <p>
  
        <ol>
          <li>Introduction of the algorithm</li>
            <ol>
              <li>Abstract theory</li>
              <li>Solution</li>
            </ol>
          <li>Showcase</li>
          <ol>
            <li>CartPole</li>
            <li>LunarLander</li>
          </ol>
          <li>Experiments:</li>
          <ol>
            <li>CG-method</li>
            <li>Trust region step</li>
          </ol>
        </ol> 
      </p>   
    <h3>Introduction of the algorithm</h3>
    <p>Constrained policy optimization (CPO) is a policy search algorithm for
	constrained Markov decision processes (CMDPs). The authors of Achiam et al. <d-cite
          key="achiam2017constrained"></d-cite> derive an update step which theoretically guarantees constraint satisfaction whilst not decreasing the reward function. In each iteration a problem of the following kind is approximately solved </p>
	<d-math block="">
      		\pi_{k+1} = \arg \max_{\pi \in \Pi_\theta} J(\pi),
    	</d-math>
    	<d-math block="">
      		\text{s.t. } J_{C_i}(\pi) \le d_i \text{ for } i=1,...,m,  \quad
      		D_{kl}(\pi, \pi_k) \le \delta ,
    	</d-math>
      <p> where <d-math>\Pi_\theta</d-math> is some set of parametrized policies, <d-math>J:\Pi_\theta \to R</d-math> is some reward function, <d-math>J_{C_i}:\Pi_\theta \to R</d-math> are some constraint functions, <d-math>d_i</d-math> are the respective limits for <d-math>i = 1,...,m</d-math> and <d-math>D_{kl}</d-math> is the KL-divergence.
          For the practical implementation the theoretical mathematical objects in the problem above, we here only mentioned vaguely, are replaced by surrogate function which can be obtained by 				samples collected from evaluating the current policy. The problem then becomes</p>
          <d-math block="">
      		\theta_{k+1} = \arg \max_{\theta} g^T (\theta - \theta_k),
        </d-math>
        <d-math block="">
      		\text{s.t. } c_i + b_i^T(\theta - \theta_k) \le 0 \text{ for } i=1,...,m, \quad
      		\frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k) \le \delta ,
    	</d-math>
    	<p>where here <d-math>g</d-math> is the gradient of the objective function, <d-math>H</d-math> is the hessian of the KL-divergence, <d-math>b_i</d-math> is the gradient of the constraint function and <d-math>c_i</d-math> is the current value of the constraint function for <d-math>i = 1,...,m</d-math>. </p>
        <p> To solve this optimization with consideration of constraints problem, we first present the 
          dual to the problem which is
        </p>
    
        <d-math block =""> 
          \max_{\lambda \geq 0, \nu \geq 0} \frac{-1}{2\lambda} (g^\top H^{-1}g - 2r^T \nu + \nu^{\top}S\nu) + \nu^\top c \frac{\lambda \delta}{2}
        </d-math>
        <p>
          Whereas
          <br>
          $B := [b_1, b_2,...]^\top, \quad$ $c :=  [c_1, c_2,...]^\top, \quad $ 
          $r := g^\top H^{-1} B, \quad $ $S := B^\top H^{-1} B$
          </p>
          Fortunately, since the number of constraints is equal to the number of dimensions in vector $\nu$, it is possible
          to solve the dual problem for an analytical solution in case we only have one constraint. Now we assume that this is 
          the case. 
          <h4>Solution to the dual problem</h4>
          Following the assumption above, we solve for $\lambda^*$ and $\nu^*$ using Lagrangian method,
           and achieve the solution to the dual problem (when it is FEASIBLE):
    
        <d-math block =""> \theta^* = \theta_k + \frac{1}{\lambda^*}H^{-1}(g - B\nu^*) </d-math>
          Whereas $\nu^* = (\frac{\lambda^*c-r}{S})_+$ and
          $\lambda^* =\arg \max
          \frac{1}{2\lambda}(\frac{r^2}{s}-q) + \frac{\lambda}{2}(\frac{c^2}{s}-q)$ if  $\lambda c - r > 0$
          <br>
          <br>
          or $\lambda^* =\arg \max\frac{1}{2}(\frac{q}{\lambda}+ \lambda \delta)$, if otherwise.

          <br>
          <br>
          When it is however, infeasible, we use the following update rule:
    
        <d-math block =""> \theta^* = \theta_k - \sqrt{\frac{2\delta}{b^\top H^{-1}b}}H^{-1}b</d-math>
    <br>
    <p> 
          For our next step, we implemented the solution to the dual problem above using Python.
          Before we showcase the results, it is clear that the algorithm might require great computational power, 
          since we deal a lot with the inverse matrix problem ($H^{-1}b$). 
          To improve the efficiency of the algorithm, we applied the CG method to solve the inverse matrix problem.
          <br>
          Furthermore, the trust region coefficients play an important role in our algorithm, since it would affect our backtrack step in case of infeasibility.
    </p>  	
    <h4> CG iteration </h4>
    
    <p> As seen in the introductory part computing the CPO update involves a lot of expressions as <d-math> H^{-1}b, H^{-1}g </d-math>.
    	Computing the inverse of a matrix is computationally very expensive, hence Achiam et al. <d-cite
          key="achiam2017constrained"></d-cite> advice the reader to circumvent it, by instead solving the linear algebraic systems <d-math> Hx = b</d-math> and <d-math> Hx = g</d-math>
          respectively. Obviously in exact arithmetics the respective solutions would give us the quantities we are looking for, but as most of us only have access to the floating point numbers in the IEEE 754 standard, 
          for this matter we will have to resort to some approximation. Here Achiam et al. <d-cite
          key="achiam2017constrained"></d-cite> propose to use the CG Algorithm, this is an iterative method for solving linear algebraic systems particularly designed for symmetric positiv-definite matrices, cf. <d-cite
          key="liesen2023"></d-cite> chapter 4.2.
    </p>
    <p> To observe the influence of the quality of the approximation of said quantities we ran the CPO algorithm with different numbers of CG-iterations.
    	Our hypothesis was that increasing the number of iterations would lead to an improvement in the learning progress.
    	Since the better approximation would lead to more accurate steps and thus 
    	the practical implemantation would be closer to the theoretically derived update.
    </p>
    	
    <d-figure id="cgits-visualization">
      <figure>
        <div id="cgits-visualization-target" class="image" style="max-width: 2000px;"></div>
        <figcaption>
          Learning progress and constraint satisfaction in the LunarLander and CartPole environment for various agents trained with different numbers of CG-iterations within each update step. <br>
          In the LunarLander environment the agent was supposed to not surpase a certain speed whilst for CartPole he was supposed to stay whithin the middle section of the screen.
        </figcaption>
      </figure>
    </d-figure>
    
    <p> To further understand the impact of the increased amount of CG-iterations we had a look at the relative forward error.
    For a linear algebraic system <d-math> Ax = b </d-math> this quantity is given by
    	<d-math>
      		\frac{||x-\hat{x}||}{||x||}
    	</d-math>
    	for some approximation <d-math> \hat{x} </d-math> and some norm <d-math> ||\cdot|| </d-math>, this value is useful to evaluate the quality of an estimation. As we do not know the exact solution <d-math> x </d-math> in most cases, we can only use bounds to assess it.
    	One such bound we are going to use, is the following</p>
    <d-math block="">
      		\frac{||x-\hat{x}||}{||x||} \le K(A) \frac{||r||}{||b||},
    	</d-math>
    <p>where <d-math> K(A) </d-math> is the condition number of <d-math> A </d-math> w.r.t. <d-math> ||\cdot|| </d-math> and <d-math> r = b - A\hat{x} </d-math> is the residual.
    This bound tells us, if the matrix <d-math> A </d-math> has a reasonable condition number a small residual implies a good approximation.</p>
    	
    <d-figure id="residual-visualization">
      <figure>
        <div id="residual-visualization-target" class="image" style="max-width: 2000px;"></div>
        <figcaption>
          Plot of the residual and bound on the relative forward error with respect to the 2-norm, here <d-math> r1 = ||b - H\hat{x}||_2 </d-math> and <d-math> r2 = ||g - H\hat{x}||_2 </d-math>.
        </figcaption>
      </figure>
    </d-figure>
    
    <p> Indeed we can see in the second graph that increasing the number of CG-iterations to fifty and above drastically lowers the residualnorm and the bound on relative forward error.
    Also for both environments for 10 and 20 CG-iterations the bound on the relative forward error sometimes lies way beyond 1, implying that the approximation might not be better than a random guess. On the other hand, in terms of the reward when looking at the first graph the impact is not noticeable, it behaves rather similar for all four different values.
    One explanation could be that our approximation of the consdition number of <d-math> H </d-math> is too pesimistic. </p>
    
    <d-figure id="constraint-visualization">
      <figure>
        <div id="constraint-visualization-target" class="image" style="max-width: 2000px;"></div>
        <figcaption>
          Direct comparison between 10 and 100 CG-iterations of the behaviour of the constraint value.
        </figcaption>
      </figure>
    </d-figure>
    
    <p> Only when considering the constraint value one could argue that increasing the number of CG-iterations leads to a somewhat more stable behaviour during training. 
    Moreover to further justify this point for oneself one would have to ignore the last 25 iterations in the LunarLander environment.
    All in all our experiments seem to indicate that the impact of the number of CG-iterations is rather subtle and does not considerably improve the training proccess,
    obviously for more reliable results more experiments would need to be conducted.</p>

    <h4> Trust region </h4>

    <p> In our introductory part, we already mentioned our hypothesis of the trust region size's effect on the model's training
<br>
      To test this hypothesis, we train the agent for $\delta \in \{0.001, 0.01, 0.05, 0.1\}$ and again for 100 iterations. 
      According to our hypothesis, the performance of a model w.r.t a large trust region size should be higher than those of the smaller size for the reward values. Its constraint
      value, however, should be higher and might not converge to the limit.
    </p>
    <p>
      In the first figure, we showcase the reward value obtained after 100 training iterations for both CartPole and 
      LunarLander environments.
    </p>
    <d-figure id="constraint-visualization-trust-region-reward">
      <figure>
        <div id="constraint-visualization-trust-region-reward-target" class="image" style="max-width: 2000px;"></div>
        <figcaption>
          Comparision of reward values for different trust region sizes
        </figcaption>
      </figure>
    </d-figure>
    <p>
      <p>
        In the second figure, we showcase the constraint value obtained after 100 training iterations for both CartPole and 
        LunarLander environments.
      </p>      
    </p>

    <d-figure id="constraint-visualization-trust-region-constraint">
      <figure>
        <div id="constraint-visualization-trust-region-constraint-target" class="image" style="max-width: 2000px;"></div>
        <figcaption>
          Comparision of constraint values for different trust region sizes
        </figcaption>
      </figure>
    </d-figure>
    <p>
      For the CartPole environment, our hypothesis seems to be true, since the agent trained with large $\delta$ managed
      to achieve a much higher reward but on the other hand, its constraint value never really managed to converge to the limit.
      <br>
      For the LunarLander environment, the result, however, does not support our hypothesis. If we take a look at the reward values
      w.r.t $\delta = 0.05$, the graph oscilitates after the 60th iteration. On the other hand, the same case does not apply for an even larger $\delta = 0.1$. Ergo, we cannot confirm our hypothesis after having analyzed the LunarLander environment. 
      Although, this problem might occur because of a mistake in our technical implementation.
    </p>
    <h4>Interactive Figures</h4>

    <p>
      Here's a dynamically instantiated "figure". We use Intersection Observers to allow loading resource-heavy
      figures only when readers scroll close to them. The code for this is in <code>src/index.js</code>.
    </p>

    <d-figure id="svelte-example-dfigure">
      <figure>
        <div id="svelte-example-target"></div>
        <figcaption>And a static figcaption. You can use citations<d-cite key="mercier2011humans"></d-cite> in this
          figcaption, but not in text added by javascript.</figcaption>
      </figure>
    </d-figure>

    <p>You can't use citation tags (<code>d-cite</code>) in figures that are dynamically loaded using Javascript.
      Distill statically
      analyzes your submission for its citations, because they need to be uploaded to indexers and organizations like <a
        href="https://www.crossref.org/">CrossRef</a> and <a href="https://scholar.google.com">Google Scholar</a>.</p>

    <p>That's it for the example article! Feel free to look at <a
        href="https://github.com/distillpub?utf8=%E2%9C%93&q=post--&type=public">implementations
        of existing Distill articles</a>, or ask for help in
      the <a href="http://slack.distill.pub">Distill Slack Community</a>.</p>

  </d-article>



  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to...
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
